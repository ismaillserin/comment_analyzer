import pandas as pd
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification
from torch.utils.data import Dataset
from transformers import Trainer, TrainingArguments, TrainerCallback
from datasets import load_dataset

# âœ… Hugging Face Dataset'i yÃ¼kle (tÃ¼m diller)
dataset = load_dataset("BaoLocTown/amazon-reviews-multi-all-languages", split="train")
df = pd.DataFrame(dataset)

# Sadece boÅŸ olmayan metin ve etiketleri al
df = df[df["text"].notna() & df["label"].notna()]

# Etiketleri kontrol et (0: negative, 1: neutral, 2: positive)
label_counts = df["label"].value_counts()
print("Etiket daÄŸÄ±lÄ±mÄ±:\n", label_counts)
print(f"âœ… Veri yÃ¼klendi. Toplam Ã¶rnek sayÄ±sÄ±: {len(df)}")

# 2. EÄŸitim ve test setine ayÄ±r
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42
)
print(f"âœ… EÄŸitim Ã¶rnekleri: {len(train_texts)}, Test Ã¶rnekleri: {len(test_texts)}")

# 3. Tokenizer
tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")

# 4. Dataset sÄ±nÄ±fÄ±
class CustomDataset(Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# 5. Dataset'leri oluÅŸtur
train_dataset = CustomDataset(train_texts, train_labels)
test_dataset = CustomDataset(test_texts, test_labels)

# 6. Modeli yÃ¼kle (3 sÄ±nÄ±f iÃ§in)
model = XLMRobertaForSequenceClassification.from_pretrained("xlm-roberta-base", num_labels=3)
print("âœ… Model yÃ¼klendi: xlm-roberta-base")

# 7. EÄŸitim ayarlarÄ±
training_args = TrainingArguments(
    output_dir="./roberta_sentiment",
    num_train_epochs=5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    evaluation_strategy="epoch",
    save_strategy="no",
    logging_dir="./logs",
    logging_steps=10,
    report_to="none"
)

# 8. Metrik fonksiyonu
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "precision": precision_score(labels, preds, average="macro"),
        "recall": recall_score(labels, preds, average="macro"),
        "f1": f1_score(labels, preds, average="macro"),
    }

# (Opsiyonel) EÄŸitim sÄ±rasÄ±nda bilgilendirme iÃ§in callback
class PrintCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        print(f"ğŸ“¢ Epoch {int(state.epoch)} tamamlandÄ±.")

# 9. Trainer'Ä± baÅŸlat
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
    callbacks=[PrintCallback()]
)

# 10. EÄŸitim
print("ğŸš€ EÄŸitim baÅŸlÄ±yor...\n")
trainer.train()

# 11. DeÄŸerlendirme
print("\nâœ… EÄŸitim tamamlandÄ±. Model test verisi Ã¼zerinde deÄŸerlendiriliyor...")
results = trainer.evaluate()
print("ğŸ“Š DeÄŸerlendirme SonuÃ§larÄ±:", results)

# 12. Kaydet
model.save_pretrained("./roberta_sentiment")
tokenizer.save_pretrained("./roberta_sentiment")
print("âœ… Model ve tokenizer kaydedildi: ./roberta_sentiment klasÃ¶rÃ¼ne.")
